{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fc61857-629d-476c-84fc-927a63a12f0f",
   "metadata": {},
   "source": [
    "# Lab 3. Create a native Agent with OpenVINO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04a1a2b-873b-4c05-a9fc-9a762ddeffe7",
   "metadata": {},
   "source": [
    "## Create LLM as agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0c9e882-5622-454e-b28c-9d799678b8cd",
   "metadata": {
    "test_replace": {
     "Qwen/Qwen2.5-3B-Instruct": "Qwen/Qwen2.5-1.5B-Instruct"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from modelscope import snapshot_download\n",
    "llm_model_id = \"snake7gun/Qwen2-7B-Instruct-int4-ov\"\n",
    "# llm_model_id = \"snake7gun/Qwen2.5-3B-Instruct-int4-ov\"\n",
    "llm_local_path  = \"./model/snake7gun/Qwen2-7B-Instruct-int4-ov\"\n",
    "# llm_local_path  = \"./model/snake7gun/Qwen2.5-3B-Instruct-int4-ov\"\n",
    "\n",
    "if not Path(llm_local_path).exists():\n",
    "    model_dir = snapshot_download(llm_model_id, cache_dir=\"./model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dcdf3a",
   "metadata": {},
   "source": [
    "### Select inference device for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74be1b36-a56d-41a7-9d1a-d487b22e5dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[ERROR] 21:31:21.463 [NPUBackends] Cannot find backend for inference. Make sure the device is available.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcab2cc680b3401a97d03d2897124c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'GPU', 'AUTO'), value='CPU')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notebook_utils import device_widget\n",
    "\n",
    "llm_device = device_widget(\"CPU\", exclude=[\"NPU\"])\n",
    "\n",
    "llm_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47dddf",
   "metadata": {},
   "source": [
    "## Instantiate LLM using Optimum Intel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "587d43fd-2f20-4ac5-9e5e-5fa0f48bed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoConfig, TextStreamer\n",
    "from transformers.generation import (\n",
    "    StoppingCriteriaList,\n",
    "    StoppingCriteria,\n",
    ")\n",
    "import openvino.properties as props\n",
    "import openvino.properties.hint as hints\n",
    "import openvino.properties.streams as streams\n",
    "\n",
    "import json\n",
    "import json5\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_local_path, trust_remote_code=True)\n",
    "\n",
    "ov_config = {hints.performance_mode(): hints.PerformanceMode.LATENCY, streams.num(): \"1\", props.cache_dir(): \"\"}\n",
    "\n",
    "llm = OVModelForCausalLM.from_pretrained(\n",
    "    llm_local_path,\n",
    "    device=llm_device.value,\n",
    "    ov_config=ov_config,\n",
    "    config=AutoConfig.from_pretrained(llm_local_path, trust_remote_code=True),\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "llm.generation_config.top_k = 1\n",
    "llm.generation_config.max_length = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2bff03",
   "metadata": {},
   "source": [
    "### Create text generation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c379d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopSequenceCriteria(StoppingCriteria):\n",
    "    \"\"\"\n",
    "    This class can be used to stop generation whenever a sequence of tokens is encountered.\n",
    "\n",
    "    Args:\n",
    "        stop_sequences (`str` or `List[str]`):\n",
    "            The sequence (or list of sequences) on which to stop execution.\n",
    "        tokenizer:\n",
    "            The tokenizer used to decode the model outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stop_sequences, tokenizer):\n",
    "        if isinstance(stop_sequences, str):\n",
    "            stop_sequences = [stop_sequences]\n",
    "        self.stop_sequences = stop_sequences\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs) -> bool:\n",
    "        decoded_output = self.tokenizer.decode(input_ids.tolist()[0])\n",
    "        return any(decoded_output.endswith(stop_sequence) for stop_sequence in self.stop_sequences)\n",
    "\n",
    "\n",
    "def text_completion(prompt: str, stop_words) -> str:\n",
    "    im_end = \"<|im_end|>\"\n",
    "    if im_end not in stop_words:\n",
    "        stop_words = stop_words + [im_end]\n",
    "    streamer = TextStreamer(tokenizer, timeout=60.0, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    stopping_criteria = StoppingCriteriaList([StopSequenceCriteria(stop_words, tokenizer)])\n",
    "    input_ids = torch.tensor([tokenizer.encode(prompt)])\n",
    "    generate_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        streamer=streamer,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "    )\n",
    "    output = llm.generate(**generate_kwargs)\n",
    "    output = output.tolist()[0]\n",
    "    output = tokenizer.decode(output, errors=\"ignore\")\n",
    "    assert output.startswith(prompt)\n",
    "    output = output[len(prompt) :].replace(\"<|endoftext|>\", \"\").replace(im_end, \"\")\n",
    "\n",
    "    for stop_str in stop_words:\n",
    "        idx = output.find(stop_str)\n",
    "        if idx != -1:\n",
    "            output = output[: idx + len(stop_str)]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fe6e0c",
   "metadata": {},
   "source": [
    "## Create prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f614efa9-2c13-45ec-89ad-1dab63f346d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOL_DESC = \"\"\"{name_for_model}: Call this tool to interact with the {name_for_human} API. What is the {name_for_human} API useful for? {description_for_model} Parameters: {parameters}\"\"\"\n",
    "\n",
    "PROMPT_REACT = \"\"\"Answer the following questions as best you can. You have access to the following APIs:\n",
    "\n",
    "{tools_text}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tools_name_text}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can be repeated zero or more times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {query}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0548431e",
   "metadata": {},
   "source": [
    "Meanwhile we have to create function for consolidate the tools information and conversation history into the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71d46914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_text(chat_history, list_of_tool_info) -> str:\n",
    "    tools_text = []\n",
    "    for tool_info in list_of_tool_info:\n",
    "        tool = TOOL_DESC.format(\n",
    "            name_for_model=tool_info[\"name_for_model\"],\n",
    "            name_for_human=tool_info[\"name_for_human\"],\n",
    "            description_for_model=tool_info[\"description_for_model\"],\n",
    "            parameters=json.dumps(tool_info[\"parameters\"], ensure_ascii=False),\n",
    "        )\n",
    "        if tool_info.get(\"args_format\", \"json\") == \"json\":\n",
    "            tool += \" Format the arguments as a JSON object.\"\n",
    "        elif tool_info[\"args_format\"] == \"code\":\n",
    "            tool += \" Enclose the code within triple backticks (`) at the beginning and end of the code.\"\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        tools_text.append(tool)\n",
    "    tools_text = \"\\n\\n\".join(tools_text)\n",
    "\n",
    "    tools_name_text = \", \".join([tool_info[\"name_for_model\"] for tool_info in list_of_tool_info])\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "    for i, (query, response) in enumerate(chat_history):\n",
    "        if list_of_tool_info:\n",
    "            if (len(chat_history) == 1) or (i == len(chat_history) - 2):\n",
    "                query = PROMPT_REACT.format(\n",
    "                    tools_text=tools_text,\n",
    "                    tools_name_text=tools_name_text,\n",
    "                    query=query,\n",
    "                )\n",
    "        if query:\n",
    "            messages.append({\"role\": \"user\", \"content\": query})\n",
    "        if response:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False, return_tensors=\"pt\")\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbed3668",
   "metadata": {},
   "source": [
    "## Create parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c9cbc6a-6114-4183-b49d-25d0ae2b5a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_latest_tool_call(text):\n",
    "    tool_name, tool_args = \"\", \"\"\n",
    "    i = text.rfind(\"\\nAction:\")\n",
    "    j = text.rfind(\"\\nAction Input:\")\n",
    "    k = text.rfind(\"\\nObservation:\")\n",
    "    if 0 <= i < j:  # If the text has `Action` and `Action input`,\n",
    "        if k < j:  # but does not contain `Observation`,\n",
    "            # then it is likely that `Observation` is ommited by the LLM,\n",
    "            # because the output text may have discarded the stop word.\n",
    "            text = text.rstrip() + \"\\nObservation:\"  # Add it back.\n",
    "        k = text.rfind(\"\\nObservation:\")\n",
    "        tool_name = text[i + len(\"\\nAction:\") : j].strip()\n",
    "        tool_args = text[j + len(\"\\nAction Input:\") : k].strip()\n",
    "        text = text[:k]\n",
    "    return tool_name, tool_args, text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b1673b",
   "metadata": {},
   "source": [
    "## Create tools calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b7b30a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"name_for_human\": \"get weather\",\n",
    "        \"name_for_model\": \"get_weather\",\n",
    "        \"description_for_model\": 'Get the current weather in a given city name.\"',\n",
    "        \"parameters\": [\n",
    "            {\n",
    "                \"name\": \"city_name\",\n",
    "                \"description\": \"City name\",\n",
    "                \"required\": True,\n",
    "                \"schema\": {\"type\": \"string\"},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name_for_human\": \"image generation\",\n",
    "        \"name_for_model\": \"image_gen\",\n",
    "        \"description_for_model\": \"AI painting (image generation) service, input text description, and return the image URL drawn based on text information.\",\n",
    "        \"parameters\": [\n",
    "            {\n",
    "                \"name\": \"prompt\",\n",
    "                \"description\": \"describe the image\",\n",
    "                \"required\": True,\n",
    "                \"schema\": {\"type\": \"string\"},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b72e3",
   "metadata": {},
   "source": [
    "Then we should implement these tools with inputs and outputs, and execute them according to the output of LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b544d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_tool(tool_name: str, tool_args: str) -> str:\n",
    "    if tool_name == \"get_weather\":\n",
    "        city_name = json5.loads(tool_args)[\"city_name\"]\n",
    "        key_selection = {\n",
    "            \"current_condition\": [\n",
    "                \"temp_C\",\n",
    "                \"FeelsLikeC\",\n",
    "                \"humidity\",\n",
    "                \"weatherDesc\",\n",
    "                \"observation_time\",\n",
    "            ],\n",
    "        }\n",
    "        resp = requests.get(f\"https://wttr.in/{city_name}?format=j1\")\n",
    "        resp.raise_for_status()\n",
    "        resp = resp.json()\n",
    "        ret = {k: {_v: resp[k][0][_v] for _v in v} for k, v in key_selection.items()}\n",
    "        return str(ret)\n",
    "    elif tool_name == \"image_gen\":\n",
    "        import urllib.parse\n",
    "\n",
    "        prompt = json5.loads(tool_args)[\"prompt\"]\n",
    "        prompt = urllib.parse.quote(prompt)\n",
    "        return json.dumps(\n",
    "            {\"image_url\": f\"https://image.pollinations.ai/prompt/{prompt}\"},\n",
    "            ensure_ascii=False,\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def llm_with_tool(prompt: str, history, list_of_tool_info=()):\n",
    "    chat_history = [(x[\"user\"], x[\"bot\"]) for x in history] + [(prompt, \"\")]\n",
    "\n",
    "    planning_prompt = build_input_text(chat_history, list_of_tool_info)\n",
    "    text = \"\"\n",
    "    while True:\n",
    "        output = text_completion(planning_prompt + text, stop_words=[\"Observation:\", \"Observation:\\n\"])\n",
    "        action, action_input, output = parse_latest_tool_call(output)\n",
    "        if action:\n",
    "            observation = call_tool(action, action_input)\n",
    "            output += f\"\\nObservation: = {observation}\\nThought:\"\n",
    "            observation = f\"{observation}\\nThought:\"\n",
    "            print(observation)\n",
    "            text += output\n",
    "        else:\n",
    "            text += output\n",
    "            break\n",
    "\n",
    "    new_history = []\n",
    "    new_history.extend(history)\n",
    "    new_history.append({\"user\": prompt, \"bot\": text})\n",
    "    return text, new_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745969ca",
   "metadata": {},
   "source": [
    "## Run agent\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "560cd122-17b8-4d04-a75b-3d048ac89ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: I need to use the get_weather API to get the weather information in London first, then use the image_gen API to generate an image based on the weather information.\n",
      "Action: get_weather\n",
      "Action Input: {\"city_name\": \"London\"}\n",
      "Observation:\n",
      "{'current_condition': {'temp_C': '16', 'FeelsLikeC': '16', 'humidity': '94', 'weatherDesc': [{'value': 'Partly cloudy'}], 'observation_time': '02:00 AM'}}\n",
      "{'current_condition': {'temp_C': '16', 'FeelsLikeC': '16', 'humidity': '94', 'weatherDesc': [{'value': 'Partly cloudy'}], 'observation_time': '02:00 AM'}}\n",
      "Thought:\n",
      " I got the weather information in London, which includes temperature, humidity, and weather conditions. Now I will use the image_gen API to generate an image based on this information.\n",
      "Action: image_gen\n",
      "Action Input: {\"prompt\": \"Big Ben under partly cloudy sky with temperature 16 degrees Celsius and humidity 94%\"}\n",
      "Observation:\n",
      "{\"image_url\": \"https://image.pollinations.ai/prompt/Big%20Ben%20under%20partly%20cloudy%20sky%20with%20temperature%2016%20degrees%20Celsius%20and%20humidity%2094%25\"}\n",
      "{\"image_url\": \"https://image.pollinations.ai/prompt/Big%20Ben%20under%20partly%20cloudy%20sky%20with%20temperature%2016%20degrees%20Celsius%20and%20humidity%2094%25\"}\n",
      "Thought:\n",
      " I successfully generated an image based on the weather information provided by the user.\n",
      "Final Answer: The current weather in London is partly cloudy with a temperature of 16 degrees Celsius and humidity of 94%. Based on this information, I created an image of Big Ben under a partly cloudy sky. You can view the image at the following URL: ![Big Ben](https://image.pollinations.ai/prompt/Big%20Ben%20under%20partly%20cloudy%20sky%20with%20temperature%2016%20degrees%20Celsius%20and%20humidity%2094%25).\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "query = \"get the weather in London, and create a picture of Big Ben based on the weather information\"\n",
    "\n",
    "response, history = llm_with_tool(prompt=query, history=history, list_of_tool_info=tools)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/openvinotoolkit/openvino_notebooks/assets/91237924/22fa5396-8381-400f-a78f-97e25d57d807",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [
     "LLM"
    ],
    "tasks": [
     "Text Generation"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
